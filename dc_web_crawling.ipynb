{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dc_web_crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "158I_GVuv79mDUk8nLiRO681Fi4yYNPkK",
      "authorship_tag": "ABX9TyPP8W9GFT4JjJQiZcb9eXqf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttury/dcinside_corpus_croller/blob/main/dc_web_crawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHXXtrf_1Sbe"
      },
      "source": [
        "# ---constant---\n",
        "\n",
        "BASE_URL_REG = \"https://gall.dcinside.com/board/lists\" # 정식갤\n",
        "ARTICLE_BASE_URL = \"https://gall.dcinside.com\" # 게시물\n",
        "NORMAL_ARTICLE_URL_REG = \"https://gall.dcinside.com/board/view/\" # 공지 제외 게시물\n",
        "\n",
        "BASE_URL_MINI = \"https://gall.dcinside.com/mini/board/lists\" # 미니갤\n",
        "NORMAL_ARTICLE_URL_MINI = \"https://gall.dcinside.com/mini/board/view/\"\n",
        "\n",
        "BASE_URL_MOBILE = \"https://m.dcinside.com/board/maplestory/\" # 모바일\n",
        "\n",
        "GALARIES = {'야갤' : 'baseball_new10', '메갤' : 'maplestory', '대고갤' : 'eowjsrhemdgkrry', '식물갤' : 'tree'}\n",
        "\n",
        "# -------------\n",
        "\n",
        "# ---hyper parameter---\n",
        "\n",
        "GALARY_ID = GALARIES['식물갤'] # 갤러리 ID\n",
        "MAX_SLEEP_TIME = 3 # 크롤링 지연 시간 -> 차단 회피\n",
        "BASE_URL = BASE_URL_REG\n",
        "NORMAL_ARTICLE_URL = NORMAL_ARTICLE_URL_REG\n",
        "\n",
        "# 헤더 설정, 차단을 피하기 위함\n",
        "headers = {\n",
        "    \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
        "    \"Accept-Encoding\" : \"gzip, deflate, br\",\n",
        "    \"Accept-Language\" : \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
        "    \"Cache-Control\" : \"max-age=0\",\n",
        "    \"Connection\" : \"keep-alive\",\n",
        "    \"Host\" : \"gall.dcinside.com\",\n",
        "    \"Referer\" : \"https://mc9w4ageo7e-496ff2e9c6d22116-0-colab.googleusercontent.com/\",\n",
        "    \"sec-ch-ua\" : 'Chromium\";v=\"90\", \" Not A;Brand\";v=\"99\", \"Whale\";v=\"2\"',\n",
        "    \"Sec-Fetch-Dest\" : 'document',\n",
        "    \"sec-ch-ua-mobile\" : \"?0\",\n",
        "    \"Sec-Fetch-Mode\" : \"navigate\",\n",
        "    \"Sec-Fetch-Site\" : \"cross-site\",\n",
        "    \"Sec-Fetch-User\" : \"?1\",\n",
        "    \"Upgrade-Insecure-Requests\" : \"1\",\n",
        "    \"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.229 Whale/2.10.123.42 Safari/537.36\",\n",
        "    }\n",
        "\n",
        "# ---------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Uv2Sr4AEj1Y"
      },
      "source": [
        "import requests\n",
        "from urllib import request\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from random import randint\n",
        "import re\n",
        "import pandas as pd\n",
        "from urllib.error import HTTPError\n",
        "from urllib.error import URLError\n",
        "\n",
        "data = []\n",
        "# 1페이지 ~ 10페이지\n",
        "for i in range(1, 2):\n",
        "  # 파라미터 설정(갤러리 설정)\n",
        "  params = {'id': GALARY_ID,'page':i}\n",
        "\n",
        "  # 갤러리 메인 접속\n",
        "  try:\n",
        "    response = requests.get(BASE_URL, params=params, headers=headers)\n",
        "  except HTTPError as e:\n",
        "    print('http error')\n",
        "    continue\n",
        "  except URLError as e:\n",
        "    print('url error')\n",
        "    continue\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # 한 페이지에서 글 목록 긁어오기\n",
        "  try:\n",
        "    article_list = soup.find('tbody').find_all('tr')\n",
        "  except AttributeError as e:\n",
        "    print(\"Tag was not found\")\n",
        "    continue\n",
        "  else:\n",
        "    if soup.find('tbody') == None:\n",
        "      print(\"Tag was not found\")\n",
        "      continue\n",
        "\n",
        "  # 글 목록에서 각 게시물 별 접근\n",
        "  for tr_item in article_list:\n",
        "\n",
        "    # 각 게시물 제목, url 추출\n",
        "    title_tag = tr_item.find('a', href=True)\n",
        "    if title_tag == None:\n",
        "      continue\n",
        "    article_title = title_tag.text\n",
        "    url = title_tag['href']\n",
        "    article_url = ARTICLE_BASE_URL + url\n",
        "\n",
        "    # 공지나 광고 같이 필요 없는 게시물은 무시\n",
        "    if article_url.find(NORMAL_ARTICLE_URL) == -1:\n",
        "      continue\n",
        "\n",
        "    print('+'*12)\n",
        "    print(\"제목: \", article_title)\n",
        "    print(\"주소: \", article_url)\n",
        "    print('+'*12)\n",
        "\n",
        "    data.append([article_title, article_url, extract_corpus(article_title, article_url, params, headers)])\n",
        "    random_time = randint(1, MAX_SLEEP_TIME)\n",
        "    time.sleep(random_time)\n",
        "\n",
        "columns = ['title', 'url', 'content']\n",
        "dc_data = pd.DataFrame(data, columns=columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paZ_lMVon30g"
      },
      "source": [
        "print(dc_data[:5])\n",
        "dc_data.to_csv('./drive/MyDrive/data/dc_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm8lfUBf7RyI"
      },
      "source": [
        "from urllib.request import urlopen, Request\n",
        "from fake_useragent import UserAgent\n",
        "\n",
        "useragent = UserAgent()\n",
        "\"\"\"\n",
        "headers = {\n",
        "    'Host': 'gall.dcinside.com',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Content-Length': '128',\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"90\", \" Not A;Brand\";v=\"99\", \"Whale\";v=\"2\"',\n",
        "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.232 Whale/2.10.124.26 Safari/537.36',\n",
        "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "    'Origin': 'https://gall.dcinside.com',\n",
        "    'Sec-Fetch-Site': 'same-origin',\n",
        "    'Sec-Fetch-Mode': 'cors',\n",
        "    'Sec-Fetch-Dest': 'empty',\n",
        "    'Referer': 'https://gall.dcinside.com/board/view/?id=dcbest&no=24366',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "}\n",
        "\"\"\"\n",
        "headers = {\n",
        "    'Host': 'gall.dcinside.com',\n",
        "    'Connection': 'keep-alive',\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.232 Whale/2.10.124.26 Safari/537.36',\n",
        "    'Referer': 'https://gall.dcinside.com/board/view/?id=dcbest&no=24366',\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"90\", \" Not A;Brand\";v=\"99\", \"Whale\";v=\"2\"',\n",
        "    'Accept': 'application/json, text/javascript, */*; q=0.01',\n",
        "    'X-Requested-With': 'XMLHttpRequest',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',\n",
        "    'Origin': 'https://gall.dcinside.com',\n",
        "    'Sec-Fetch-Site': 'same-origin',\n",
        "    'Sec-Fetch-Mode': 'cors',\n",
        "    'Sec-Fetch-Dest': 'empty',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',\n",
        "}\n",
        "comment_url = 'https://gall.dcinside.com/board/comment/'\n",
        "comment_response = requests.post(comment_url, headers=headers)\n",
        "\n",
        "# print(comment_corpus[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1_UCdcn4iBV"
      },
      "source": [
        "pip install fake_useragent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53pasIB42oGw"
      },
      "source": [
        "import json\n",
        "json_data = comment_response.text\n",
        "print(json_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vNJcNHot3g-"
      },
      "source": [
        "def extract_comment(article_soup):\n",
        "\n",
        "  # 게시물에서 댓글 부분 긁어오기\n",
        "  try:\n",
        "    article_comments = article_soup.find('div', class_=\"comment_box\")\n",
        "  except AttributeError as e:\n",
        "    print(\"Article Error\")\n",
        "    return\n",
        "  else:\n",
        "    if article_comments == None:\n",
        "      print(\"Comments tag not found\")\n",
        "      return\n",
        "  \n",
        "  # 게시물 댓글에서 텍스트만 뽑아내기\n",
        "  comments = []\n",
        "  comment_group = []\n",
        "  # comments 구조 : comments = [[comment, reply, reply], [comment], [comment, reply]]\n",
        "  for article_comment in article_comments:\n",
        "    # 댓글\n",
        "    if article_comment.attrs['class'] == 'ub-content':\n",
        "      comments.append(comment_group)\n",
        "      comment = article_comment.find_all(p, class_='usertxt ub-word')\n",
        "      comment_text = str(comment)\n",
        "      comment_text = comment_text.replace('<br/>', '\\n')\n",
        "      comment_text = comment_text.replace('<br>', '\\n')\n",
        "      comment_text_soup = BeautifulSoup(comment_text)\n",
        "      comment_text = comment_text_soup.get_text()\n",
        "      comment_group = [comment_text]\n",
        "    else:\n",
        "      if article_comment.find(p, class_='usertxt ub-word') == Null:\n",
        "        continue\n",
        "      else:\n",
        "        reply = article_comment.find(p, class_='usertxt ub-word')\n",
        "        reply_text = str(reply)\n",
        "        reply_text = reply_text.replace('<br/>', '\\n')\n",
        "        reply_text = reply_text.replace('<br>', '\\n')\n",
        "        reply_text_soup = BeautifulSoup(reply_text)\n",
        "        reply_text = reply_text_soup.get_text()\n",
        "        comment_group.append(reply_text)\n",
        "  \n",
        "  return comments\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkzh2zIR1qSY"
      },
      "source": [
        "def extract_corpus(article_title, article_url, params, headers):\n",
        "    # 게시물 긁어오기\n",
        "    try:\n",
        "      article_response = requests.get(article_url, params=params, headers=headers)\n",
        "    except HTTPError as e:\n",
        "      print('http error')\n",
        "      return\n",
        "    except URLError as e:\n",
        "      print('url error')\n",
        "      return\n",
        "    article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
        "    \n",
        "    article_corpus = extract_content(article_soup)\n",
        "    return article_corpus\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI_bPuyt42b_"
      },
      "source": [
        "def extract_content(article_soup):\n",
        "  \n",
        "    # 게시물에서 본문 부분 긁어오기\n",
        "    try:\n",
        "      article_contents = article_soup.find('div', class_=\"write_div\")\n",
        "    except AttributeError as e:\n",
        "      print(\"Article Error\")\n",
        "      return\n",
        "    else:\n",
        "      if article_contents == None:\n",
        "        print('Contents tag not found')\n",
        "        return\n",
        "    \n",
        "    # dc 앱에서 작성한 글에 붙어 있는 헤더 삭제\n",
        "    dcapp_tags = article_contents.find_all('span', id='dcappheader')\n",
        "    for dcapp_tag in dcapp_tags:\n",
        "      dcapp_tag.extract()\n",
        "    \n",
        "    # 게시물 본문에서 텍스트만 뽑아내기\n",
        "    print(\"-------------\")\n",
        "    # 중복된 텍스트는 처리하지 않음\n",
        "    previous_text = ''\n",
        "    article_corpus = ''\n",
        "    for article_content in article_contents:\n",
        "\n",
        "      # <br> 태그를 줄바꿈으로 변환\n",
        "      try:\n",
        "        article_text = str(article_content)\n",
        "        article_text = article_text.replace('<br/>', '\\n')\n",
        "        article_text = article_text.replace('<br>', '\\n')\n",
        "        article_text_soup = BeautifulSoup(article_text)\n",
        "        article_text = article_text_soup.get_text()\n",
        "      except:\n",
        "        continue\n",
        "\n",
        "      if article_text == previous_text:\n",
        "        continue\n",
        "\n",
        "      article_corpus += (article_text + \"\\n\")\n",
        "      previous_text = article_text\n",
        "\n",
        "    article_corpus = re.sub(r'- dc official App', '', article_corpus) \n",
        "    article_corpus = re.sub(r'\\n+', '\\n', article_corpus)\n",
        "    article_corpus = re.sub(r'^\\n+', '', article_corpus)\n",
        "    article_corpus = re.sub(r'\\n+$', '', article_corpus)\n",
        "    print(article_corpus)\n",
        "    print(\"-------------\\n\")\n",
        "    return article_corpus"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}